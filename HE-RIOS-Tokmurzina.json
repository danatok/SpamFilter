{"paragraphs":[{"text":"%md\n\n## Spark Project\n#### Exploring the Ling-Spam email dataset\n\n**Language**: Scala\n**Requirements**: \n- [HDP 2.6.5](http://hortonworks.com/products/sandbox/)\n- Spark 2.x\n\n**Authors**: Cristina Chenchen Dana\n\n\nThe following report describes the implementation of the Spam Filter which is developed by using Spark and Scala and implemented in Zeppelin notebook. The project covers the techniques that we have learned during the big data class so far, especially how to optimize the processing of big data (by parallization) using HDP Sandbox. As for the data set, the project used ling-spam data set which contains 2412 ham and 481 spam emails. This project aims to find the most informative words (the top words) which can be exploited to distinguish spam and ham emails.","user":"anonymous","dateUpdated":"2018-10-27T13:03:39+0000","config":{"tableHide":true,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":false,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Spark Project</h2>\n<h4>Exploring the Ling-Spam email dataset</h4>\n<p><strong>Language</strong>: Scala\n<br  /><strong>Requirements</strong>:</p>\n<ul>\n<li><a href=\"http://hortonworks.com/products/sandbox/\">HDP 2.6.5</a></li>\n<li>Spark 2.x</li>\n</ul>\n<p><strong>Authors</strong>: Cristina Chenchen Dana</p>\n<p>The following report describes the implementation of the Spam Filter which is developed by using Spark and Scala and implemented in Zeppelin notebook. The project covers the techniques that we have learned during the big data class so far, especially how to optimize the processing of big data (by parallizationn) using HDP Sandbox. As for the data set, the project used ling-spam data set which contains 2412 ham and 481 spam emails. This project aims to find the most informative words (the top words) which can be exploited to distinguish spam and ham emails.</p>\n"}]},"apps":[],"jobName":"paragraph_1540389903218_-510382821","id":"20160410-003138_1880368561","dateCreated":"2018-10-24T14:05:03+0000","dateStarted":"2018-10-27T13:03:08+0000","dateFinished":"2018-10-27T13:03:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:990"},{"title":"Import the libraries","text":"//The first step is to import the Spark packages.\n\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf\nimport org.apache.spark.rdd.RDD","user":"anonymous","dateUpdated":"2018-10-27T13:03:08+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf\nimport org.apache.spark.rdd.RDD\n"}]},"apps":[],"jobName":"paragraph_1540389903240_-434202539","id":"20180923-211737_1641560190","dateCreated":"2018-10-24T14:05:03+0000","dateStarted":"2018-10-27T13:03:10+0000","dateFinished":"2018-10-27T13:03:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:991"},{"title":"Function \"probaWordDir\"","text":"def probaWordDir(sc:SparkContext)(filesDir:String)\r\n  :(RDD[(String, Double)], Long) = { \r\n    \r\n    //this function is used to  calculate the probability of occurrence of a word in its given class. The function will return an RDD[(word, the probability of its occurrence)], Long[the number of files in the given directory]\r\n      \r\n    \r\n    // read all the txt files\r\n    // it will return: RDD[String, String]=[The name of the file(.txt), Then content of the file]\r\n    // \"sc\" variable is a SparkContext object which tells Spark how to access a cluster\r\n    val files = sc.wholeTextFiles(filesDir)\r\n    \r\n    // count the number of files\r\n    // it will return: Long = number of the files\r\n    val nbFiles = files.count\r\n    \r\n    // get the distinct words which are in txt files, since the goal is only to know whether the word appears or not into an email\r\n    // it will return: RDD[Array(Array(String))]\r\n    val wordSeparate = files.map(f => f._2.split(\"\\\\s+\").distinct)  \r\n    \r\n    //Remove the noninformative words\r\n    //Create array with non informative words, and here we use '\\\\' to stand for '\\'. RDD[Array(String)].\r\n    val filteredKeyWords = Array(\".\", \":\", \",\", \" \", \"/\", \"\\\\\", \"-\", \"'\", \"(\", \")\", \"@\")\r\n    \r\n     // Mapping result into filter function to eliminate non-informative words. It will return:\r\n    // it will return: RDD[Array(Array(String))]\r\n     val filteredSeparateWords = wordSeparate.map(list => list.filter(str => !filteredKeyWords.contains(str)))\r\n    \r\n    // We use flatMap to flatten the result\r\n    // it will return RDD[Array(String)]\r\n    val flatMapWord = filteredSeparateWords.flatMap(list=>list)\r\n    \r\n    // we use mapreducer to count number of occurrences of the specific word. ReduceByKey allows us to compare only keys and accumulate values. Word is the key, number of occurences is the value.\r\n    // it will return RDD[Array(String,Int)] \"String\" represents the word, \"Int\" is the occurrence\r\n    val wordDirOccurency = flatMapWord.map(word => (word,1)).reduceByKey(_+_)\r\n    \r\n    // we calculated the probability of occurrence of the word\r\n    // it will return RDD[Array(String,Double)] \"String\" represents the word, \"Double\" is the probability\r\n    val probaWord = wordDirOccurency.map(item => (item._1, item._2.toDouble / nbFiles ))\r\n    return (probaWord, nbFiles) // couple returned by the function (RDD[(String, Double)], Long)\r\n }","user":"anonymous","dateUpdated":"2018-10-27T13:24:39+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"probaWordDir: (sc: org.apache.spark.SparkContext)(filesDir: String)(org.apache.spark.rdd.RDD[(String, Double)], Long)\n"}]},"apps":[],"jobName":"paragraph_1540389903242_-433433041","id":"20180923-211835_549575843","dateCreated":"2018-10-24T14:05:03+0000","dateStarted":"2018-10-27T13:03:14+0000","dateFinished":"2018-10-27T13:03:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:992"},{"title":"Function \"computeMutualInformationFactor\"","text":"def computeMutualInformationFactor(\r\nprobaWC:RDD[(String, Double)],\r\nprobaW:RDD[(String, Double)],\r\nprobaC: Double,\r\nprobaDefault: Double //default value when a probability is missing\r\n):RDD[(String, Double)] = {\r\n\r\n \r\n\r\n// This function returns the factor of each word (so it returns a RDD) given a class value (spam or ham) and an occurrence value (true or false). RDD[(String, Double)] = (word, MIfactor)\r\n//probaWC is a RDD with the map structure:word=>probability the word occurs in an email of a given class\r\n//probaW has the map structure: word=> probability the word occurs (whatever the class is)。It is （trueHam+ trueSpam）\r\n//probaC is the probability that an email belongs to the given class\r\n\r\n// we use leftOuterJoin to combine two probabilities with a unique key word: [word, (probaW,Some （probaWC）)]. We'll take the left join of probaW (proba of the word occurs in a file) with probaWC (proba word occurs in a file of a class). Left Outer join allows to create RDD with unique keys\r\n \r\nval probWJoin: RDD[(String, (Double, Option[Double]))] = probaW.leftOuterJoin(probaWC)\r\n//when a word does not occur in both classes but only one, its probability P(occurs,class) must take the probaDefault (0.2 instead of 0). We replace the None value using geOrElse function. \r\n\r\n\r\nval valueClassAndOcu: RDD[(String, (Double, Double))] = probWJoin.map(x => (x._1, (x._2._1, x._2._2.getOrElse(probaDefault))))\r\n\r\n//In order to have log we change ln to log2 (by using ln(x)/ln(2)=log2(x)\r\n// according to the formula of MI\r\n\r\nvalueClassAndOcu.map(x => (x._1, x._2._2 * (math.log(x._2._2 / (x._2._1 * probaC)) / math.log(2.0))))\r\n}\r\n    \r\n","user":"anonymous","dateUpdated":"2018-10-27T13:15:50+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"computeMutualInformationFactor: (probaWC: org.apache.spark.rdd.RDD[(String, Double)], probaW: org.apache.spark.rdd.RDD[(String, Double)], probaC: Double, probaDefault: Double)org.apache.spark.rdd.RDD[(String, Double)]\n"}]},"apps":[],"jobName":"paragraph_1540389903243_-433817790","id":"20180923-211901_247460133","dateCreated":"2018-10-24T14:05:03+0000","dateStarted":"2018-10-27T13:15:52+0000","dateFinished":"2018-10-27T13:15:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:993"},{"title":"Read the data","text":"// Read the directories containing the data\n\nval (pPresentGivenSpam, nSpam) = probaWordDir(sc)(\"/tmp/ling-spam/spam/*.txt\")\nval (pPresentGivenHam, nHam) = probaWordDir(sc)(\"/tmp/ling-spam/ham/*.txt\")\n    ","user":"anonymous","dateUpdated":"2018-10-27T13:03:15+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"pPresentGivenSpam: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[36] at map at <console>:76\nnSpam: Long = 481\npPresentGivenHam: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[44] at map at <console>:76\nnHam: Long = 2412\n"}]},"apps":[],"jobName":"paragraph_1540389903244_-435741535","id":"20180923-212047_1364377930","dateCreated":"2018-10-24T14:05:03+0000","dateStarted":"2018-10-27T13:03:27+0000","dateFinished":"2018-10-27T13:05:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:994"},{"title":"Main","text":"// occursWords is the probability the word occurs (whatever the class is)。 Take all words\r\nval (occursWords, nbfiles) = probaWordDir(sc)(\"/tmp/ling-spam/*/*.txt\")\r\n// nbFiles: total number of the files\r\nval nbFiles = (nHam + nSpam).toDouble \r\n\r\n//probaC here we consider procC as prob of email being spam. Email can be either Spam or Ham so probHam = 1- probC. \r\nval probaC = nSpam / nbFiles\r\nval probaDefault = 0.2/nbFiles\r\n\r\n//In order to compute probWC, we multiply probaWordDir * probC. ProbWC for 1 word can be of 4 values: trueHam is (probaWordHam*probaHam), trueSpam (probaWordSpam*probaSpam), falseHam( (1-probaWordHam) *probaHam),falseSpam((1-probaWordSpam) *probaSpam). So there can be case when the probWordHam is 100% so it is 1, it appears in every single mail, which means it has for falseHam, probability 0. That causes one of probWC to be 0 and when inputting it to MIfactor function, o value cause ERROR in LOG function， as log0 notpossible. However also the word can be informative too. We get RDD [(String, Double)] \r\nval trueHam = pPresentGivenHam.map(e => (e._1, e._2 * (1-probaC))) // 1-probaC gives Prob of Ham emails\r\nval trueSpam = pPresentGivenSpam.map(e => (e._1, e._2 * probaC))\r\nval falseHam = pPresentGivenHam.map(e => (e._1, (1.0 - e._2) * (1-probaC)))\r\nval falseSpam = pPresentGivenSpam.map(e => (e._1, (1.0 - e._2) * probaC))\r\n\r\n//we are computing the false cases, when there is no word in a given class.The probability is 1-prob occurWords\r\nval noOccursWords = occursWords.map(e => (e._1, (1.0 - e._2)))\r\n\r\n//We are creating a List of factors for each case. \r\nval mutualInformationMap = List(\r\n\r\n\r\ncomputeMutualInformationFactor(trueHam, occursWords, 1-probaC, probaDefault),\r\ncomputeMutualInformationFactor(trueSpam, occursWords, probaC, probaDefault),\r\ncomputeMutualInformationFactor(falseHam, noOccursWords, 1-probaC, probaDefault),\r\ncomputeMutualInformationFactor(falseSpam, noOccursWords, probaC, probaDefault)\r\n\r\n)\r\n\r\n//In order to compute the total MI for each word, we are summing 4 factors together.\r\nval mutualInformationReduce = mutualInformationMap.reduce {(e1, e2) => e1.join(e2).map(e => (e._1, e._2._1 + e._2._2))}\r\n//More explanation: We are reducing the list of MI factors for the given word. In the every step in the list ((word, MI1) (word, MI2), (word, MI3),(word, MI4)) we take the first two elements of the list, i. e for the first iteration (word,MI1) as e1 and as e2 the (word, MI2) and join them. We obtain (word, (MI1, MI2)) structure. Next we map the obtained results into (word, (MI1 + MI2)). That becomes in the next iteration as e1 and so on.\r\n\r\n//we are ranking the 20 most informative words by the totalMI values-\r\nval topWords = mutualInformationReduce.sortBy(- _._2).take(20)\r\n\r\ntopWords.foreach(println)\r\n\r\nsc.parallelize(topWords).saveAsTextFile(\"/tmp/Chenchen/topWords.txt\") //saving result。 The process is parallelized。","user":"anonymous","dateUpdated":"2018-10-27T13:16:02+0000","config":{"editorSetting":{"editOnDblClick":false,"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"occursWords: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[65] at map at <console>:76\nnbfiles: Long = 2893\nnbFiles: Double = 2893.0\nprobaC: Double = 0.16626339440027654\nprobaDefault: Double = 6.913238852402352E-5\ntrueHam: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[66] at map at <console>:41\ntrueSpam: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[67] at map at <console>:39\nfalseHam: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[68] at map at <console>:39\nfalseSpam: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[69] at map at <console>:39\nnoOccursWords: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[70] at map at <console>:39\nmutualInformationMap: List[org.apache.spark.rdd.RDD[(String, Double)]] = List(MapPartitionsRDD[75] at map at <console>:64, MapPartitionsRDD[80] at map at <console>:64, MapPartitionsRDD[85] at map at <console>:64, MapPartitionsRDD[90] at map at <console>:64)\nmutualInformationReduce: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[102] at map at <console>:39\ntopWords: Array[(String, Double)] = Array((!,0.21351046542475682), (language,0.2102673901051873), (remove,0.16418801350461398), (free,0.15487287220247425), (university,0.15431031367461034), (money,0.1086605083210275), (click,0.09936703552670348), (our,0.08504818099156092), (today,0.07913540225683902), (sell,0.07769662496258772), (english,0.07719762981969305), (business,0.07645290117835574), (market,0.07571832215435784), (product,0.07352305324340025), (million,0.07247117877242847), (linguistics,0.07199297243911873), (internet,0.07176111648665146), (company,0.07129535268393317), (%,0.07055058358417651), (save,0.07013508171475248))\n(!,0.21351046542475682)\n(language,0.2102673901051873)\n(remove,0.16418801350461398)\n(free,0.15487287220247425)\n(university,0.15431031367461034)\n(money,0.1086605083210275)\n(click,0.09936703552670348)\n(our,0.08504818099156092)\n(today,0.07913540225683902)\n(sell,0.07769662496258772)\n(english,0.07719762981969305)\n(business,0.07645290117835574)\n(market,0.07571832215435784)\n(product,0.07352305324340025)\n(million,0.07247117877242847)\n(linguistics,0.07199297243911873)\n(internet,0.07176111648665146)\n(company,0.07129535268393317)\n(%,0.07055058358417651)\n(save,0.07013508171475248)\norg.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://sandbox-hdp.hortonworks.com:8020/tmp/Chenchen/topWords.txt already exists\n  at org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n  at org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:283)\n  at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)\n  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1493)\n  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1472)\n  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1472)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1472)\n  ... 50 elided\n"}]},"apps":[],"jobName":"paragraph_1540389903244_-435741535","id":"20161018-143604_1206436852","dateCreated":"2018-10-24T14:05:03+0000","dateStarted":"2018-10-27T13:16:02+0000","dateFinished":"2018-10-27T13:22:44+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:995"},{"user":"anonymous","dateUpdated":"2018-10-27T12:53:32+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1540389903246_-434972037","id":"20181022-174623_799514719","dateCreated":"2018-10-24T14:05:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:996"}],"name":"HE_RIOS_Tokmurzina","id":"2DSUSXAJJ","angularObjects":{"2CHS8UYQQ:shared_process":[],"2C8A4SZ9T_livy2:shared_process":[],"2CK8A9MEG:shared_process":[],"2C4U48MY3_spark2:shared_process":[],"2CKAY1A8Y:shared_process":[],"2CKEKWY8Z:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}